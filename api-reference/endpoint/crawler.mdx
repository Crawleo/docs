---
title: "Crawler API"
description: "Direct URL crawling with JavaScript rendering support"
api: "GET /crawl"
---

## Overview

The Crawler API performs direct crawling of specified URLs with JavaScript rendering support. Ideal for extracting content from single pages or multiple URLs in a single request.

## Endpoint

```
GET https://api.crawleo.dev/crawl
```

## Parameters

### Required Parameters

<ParamField query="urls" type="string" required>
  URL(s) to crawl. Can be a single URL or comma-separated list.
  
  Example: `https://example.com` or `https://example.com,https://example.org`
</ParamField>

### Output Format Parameters

<ParamField query="output_format" type="string" default="raw_html">
  Output format for crawled content. Options:
  - `raw_html` - Original HTML source
  - `enhanced_html` - Clean HTML with ads, scripts, and tracking removed
  - `markdown` - Structured Markdown (recommended for RAG/LLM)
</ParamField>

### Localization Parameters

<ParamField query="country" type="string" default="us">
  2-letter country code for geo-targeted crawling (e.g., `us`, `gb`, `de`).
</ParamField>

### Advanced Options

<ParamField query="screenshot" type="boolean" default="false">
  Capture a screenshot of the page.
</ParamField>

<ParamField query="use_proxies" type="boolean" default="false">
  Use datacenter proxies for the request.
</ParamField>

<ParamField query="use_premium_proxies" type="boolean" default="false">
  Use residential proxies for the request (higher success rate for protected sites).
</ParamField>

<Warning>
  `use_proxies` and `use_premium_proxies` are mutually exclusive. Only one can be set to `true` at a time.
</Warning>

## Example Requests

### Basic Crawl with Markdown Output

<CodeGroup>
```bash cURL
curl -X GET "https://api.crawleo.dev/crawl?urls=https://example.com&output_format=markdown" \
  -H "Authorization: Bearer YOUR_API_KEY"
```

```python Python
import requests

response = requests.get(
    "https://api.crawleo.dev/crawl",
    params={
        "urls": "https://example.com",
        "output_format": "markdown"
    },
    headers={"Authorization": "Bearer YOUR_API_KEY"}
)

data = response.json()
```

```javascript JavaScript
const response = await fetch(
  "https://api.crawleo.dev/crawl?urls=https://example.com&output_format=markdown",
  {
    headers: { "Authorization": "Bearer YOUR_API_KEY" }
  }
);

const data = await response.json();
```
</CodeGroup>

### Crawl Multiple URLs

<CodeGroup>
```bash cURL
curl -X GET "https://api.crawleo.dev/crawl?urls=https://example.com,https://example.org&output_format=markdown" \
  -H "Authorization: Bearer YOUR_API_KEY"
```

```python Python
import requests

urls = [
    "https://example.com",
    "https://example.org"
]

response = requests.get(
    "https://api.crawleo.dev/crawl",
    params={
        "urls": ",".join(urls),
        "output_format": "markdown"
    },
    headers={"Authorization": "Bearer YOUR_API_KEY"}
)

data = response.json()
```

```javascript JavaScript
const urls = [
  "https://example.com",
  "https://example.org"
].join(",");

const response = await fetch(
  `https://api.crawleo.dev/crawl?urls=${encodeURIComponent(urls)}&output_format=markdown`,
  {
    headers: { "Authorization": "Bearer YOUR_API_KEY" }
  }
);

const data = await response.json();
```
</CodeGroup>

### Get Raw HTML Only

<CodeGroup>
```bash cURL
curl -X GET "https://api.crawleo.dev/crawl?urls=https://example.com&output_format=raw_html" \
  -H "Authorization: Bearer YOUR_API_KEY"
```

```python Python
import requests

response = requests.get(
    "https://api.crawleo.dev/crawl",
    params={
        "urls": "https://example.com",
        "output_format": "raw_html"
    },
    headers={"Authorization": "Bearer YOUR_API_KEY"}
)

data = response.json()
```
</CodeGroup>

### Crawl with Proxy Support

<CodeGroup>
```bash cURL
curl -X GET "https://api.crawleo.dev/crawl?urls=https://example.com&output_format=markdown&use_premium_proxies=true&country=gb" \
  -H "Authorization: Bearer YOUR_API_KEY"
```

```python Python
import requests

response = requests.get(
    "https://api.crawleo.dev/crawl",
    params={
        "urls": "https://example.com",
        "output_format": "markdown",
        "use_premium_proxies": True,
        "country": "gb"
    },
    headers={"Authorization": "Bearer YOUR_API_KEY"}
)

data = response.json()
```
</CodeGroup>

## Response

A successful response returns crawled content for each URL:

```json
{
  "results": [
    {
      "url": "https://example.com",
      "status": 200,
      "markdown": "# Example Domain\n\nThis domain is for use in illustrative examples...",
      "raw_html": "<!DOCTYPE html><html>...</html>"
    }
  ]
}
```

<ResponseField name="results" type="array">
  Array of crawl result objects.
  
  <Expandable title="Result object properties">
    <ResponseField name="url" type="string">
      The crawled URL.
    </ResponseField>
    <ResponseField name="status" type="integer">
      HTTP status code of the crawled page.
    </ResponseField>
    <ResponseField name="raw_html" type="string">
      Full HTML source (if `raw_html=true`).
    </ResponseField>
    <ResponseField name="markdown" type="string">
      Markdown content (if `markdown=true`).
    </ResponseField>
    <ResponseField name="error" type="string">
      Error message if the crawl failed for this URL.
    </ResponseField>
  </Expandable>
</ResponseField>

## Use Cases

<AccordionGroup>
  <Accordion title="RAG Data Ingestion" icon="database">
    Crawl documentation pages or knowledge bases and convert to Markdown for vector database ingestion.
    
    ```python
    # Example: Crawl docs for RAG
    response = requests.get(
        "https://api.crawleo.dev/crawl",
        params={
            "urls": "https://docs.example.com/guide,https://docs.example.com/api",
            "output_format": "markdown"
        },
        headers={"Authorization": "Bearer YOUR_API_KEY"}
    )
    
    for result in response.json()["results"]:
        # Add to vector database
        vector_db.add(result["markdown"], metadata={"url": result["url"]})
    ```
  </Accordion>
  
  <Accordion title="Content Extraction" icon="file-lines">
    Extract clean content from web pages for analysis or processing.
  </Accordion>
  
  <Accordion title="Web Scraping" icon="spider">
    Scrape multiple pages in a single API call with JavaScript rendering support.
  </Accordion>
  
  <Accordion title="AI Agent Tools" icon="robot">
    Provide AI agents with the ability to read and understand web pages.
  </Accordion>
</AccordionGroup>

## Tips

<Tip>
  **For LLM applications**, always use `output_format=markdown` to get clean, structured content that minimizes token usage.
</Tip>

<Warning>
  Ensure you have permission to crawl the target URLs. Respect robots.txt and website terms of service.
</Warning>
