---
title: "Crawler API"
description: "Direct URL crawling with JavaScript rendering support"
api: "GET /crawler"
---

## Overview

The Crawler API performs direct crawling of specified URLs with JavaScript rendering support. Ideal for extracting content from single pages or multiple URLs in a single request.

## Endpoint

```
GET https://api.crawleo.dev/api/v1/crawler
```

## Parameters

### Required Parameters

<ParamField query="urls" type="string" required>
  Comma-separated list of URLs to crawl.
  
  Example: `https://example.com,https://example.org`
</ParamField>

### Output Format Parameters

<ParamField query="raw_html" type="boolean" default="false">
  Return the original HTML source of each page.
</ParamField>

<ParamField query="markdown" type="boolean" default="false">
  Return content as structured Markdown (recommended for RAG/LLM).
</ParamField>

## Example Requests

### Basic Crawl with Markdown Output

<CodeGroup>
```bash cURL
curl -X GET "https://api.crawleo.dev/api/v1/crawler?urls=https://example.com&markdown=true" \
  -H "Authorization: Bearer YOUR_API_KEY"
```

```python Python
import requests

response = requests.get(
    "https://api.crawleo.dev/api/v1/crawler",
    params={
        "urls": "https://example.com",
        "markdown": True
    },
    headers={"Authorization": "Bearer YOUR_API_KEY"}
)

data = response.json()
```

```javascript JavaScript
const response = await fetch(
  "https://api.crawleo.dev/api/v1/crawler?urls=https://example.com&markdown=true",
  {
    headers: { "Authorization": "Bearer YOUR_API_KEY" }
  }
);

const data = await response.json();
```
</CodeGroup>

### Crawl Multiple URLs

<CodeGroup>
```bash cURL
curl -X GET "https://api.crawleo.dev/api/v1/crawler?urls=https://example.com,https://example.org&markdown=true&raw_html=true" \
  -H "Authorization: Bearer YOUR_API_KEY"
```

```python Python
import requests

urls = [
    "https://example.com",
    "https://example.org"
]

response = requests.get(
    "https://api.crawleo.dev/api/v1/crawler",
    params={
        "urls": ",".join(urls),
        "markdown": True,
        "raw_html": True
    },
    headers={"Authorization": "Bearer YOUR_API_KEY"}
)

data = response.json()
```

```javascript JavaScript
const urls = [
  "https://example.com",
  "https://example.org"
].join(",");

const response = await fetch(
  `https://api.crawleo.dev/api/v1/crawler?urls=${encodeURIComponent(urls)}&markdown=true&raw_html=true`,
  {
    headers: { "Authorization": "Bearer YOUR_API_KEY" }
  }
);

const data = await response.json();
```
</CodeGroup>

### Get Raw HTML Only

<CodeGroup>
```bash cURL
curl -X GET "https://api.crawleo.dev/api/v1/crawler?urls=https://example.com&raw_html=true" \
  -H "Authorization: Bearer YOUR_API_KEY"
```

```python Python
import requests

response = requests.get(
    "https://api.crawleo.dev/api/v1/crawler",
    params={
        "urls": "https://example.com",
        "raw_html": True
    },
    headers={"Authorization": "Bearer YOUR_API_KEY"}
)

data = response.json()
```
</CodeGroup>

## Response

A successful response returns crawled content for each URL:

```json
{
  "results": [
    {
      "url": "https://example.com",
      "status": 200,
      "markdown": "# Example Domain\n\nThis domain is for use in illustrative examples...",
      "raw_html": "<!DOCTYPE html><html>...</html>"
    }
  ]
}
```

<ResponseField name="results" type="array">
  Array of crawl result objects.
  
  <Expandable title="Result object properties">
    <ResponseField name="url" type="string">
      The crawled URL.
    </ResponseField>
    <ResponseField name="status" type="integer">
      HTTP status code of the crawled page.
    </ResponseField>
    <ResponseField name="raw_html" type="string">
      Full HTML source (if `raw_html=true`).
    </ResponseField>
    <ResponseField name="markdown" type="string">
      Markdown content (if `markdown=true`).
    </ResponseField>
    <ResponseField name="error" type="string">
      Error message if the crawl failed for this URL.
    </ResponseField>
  </Expandable>
</ResponseField>

## Use Cases

<AccordionGroup>
  <Accordion title="RAG Data Ingestion" icon="database">
    Crawl documentation pages or knowledge bases and convert to Markdown for vector database ingestion.
    
    ```python
    # Example: Crawl docs for RAG
    response = requests.get(
        "https://api.crawleo.dev/api/v1/crawler",
        params={
            "urls": "https://docs.example.com/guide,https://docs.example.com/api",
            "markdown": True
        },
        headers={"Authorization": "Bearer YOUR_API_KEY"}
    )
    
    for result in response.json()["results"]:
        # Add to vector database
        vector_db.add(result["markdown"], metadata={"url": result["url"]})
    ```
  </Accordion>
  
  <Accordion title="Content Extraction" icon="file-lines">
    Extract clean content from web pages for analysis or processing.
  </Accordion>
  
  <Accordion title="Web Scraping" icon="spider">
    Scrape multiple pages in a single API call with JavaScript rendering support.
  </Accordion>
  
  <Accordion title="AI Agent Tools" icon="robot">
    Provide AI agents with the ability to read and understand web pages.
  </Accordion>
</AccordionGroup>

## Tips

<Tip>
  **For LLM applications**, always use `markdown=true` to get clean, structured content that minimizes token usage.
</Tip>

<Warning>
  Ensure you have permission to crawl the target URLs. Respect robots.txt and website terms of service.
</Warning>
